% Deep Knowledge Tracing is an implicit dynamic multidimensional item response theory model
% \hspace{2mm} \alert{Jill-Jênn Vie} \hspace{11mm} Hisashi Kashima\medskip\newline \includegraphics[height=9mm]{figures/inria.png}\qquad\includegraphics[height=9mm]{figures/kyoto.png}
% December 7, 2023
---
aspectratio: 169
theme: metropolis
themeoptions:
    - sectionpage=none
section-titles: false
handout: true
biblio-style: authoryear
header-includes:
    - \usepackage{booktabs}
    - \usepackage{multicol}
    - \usepackage{bm}
    - \usepackage{multirow}
    - \DeclareMathOperator\logit{logit}
    - \def\ReLU{\textnormal{ReLU}}
    - \newcommand\mycite[3]{\textcolor{blue}{#1} "#2".~#3.}
    - \usepackage{subfig}
biblatexoptions:
    - maxbibnames=99
    - maxcitenames=5
---
# Introduction

## Outline

- Explaining the title
    - Knowledge tracing
    - Strong baseline: Rasch model in item response theory [@rasch1960studies]
    - Multidimensional item response theory
    - Deep knowledge tracing [@piech2015deep]
- A few surprising recent works
- Answering those questions

Key takeaway message: deep learning is not necessarily a black box.

## Knowledge Tracing

Data
:   
A population of students answering questions  

Events
:   
"Student $i$ answered question $j$ correctly/incorrectly"

Use data from other students to predict performance for a new student:

![](figures/dkt.png)

Why is it important? If we can predict student performance, we can optimize instruction (avoid putting students in failure, etc.)

Strong baseline: IRT


## Code

```python
import sklearn
```

Our team at Inria is developing scikit-learn library (1.4 million downloads °per day°)

## Dummy dataset

\begin{columns}
\begin{column}{0.6\linewidth}
\begin{itemize}
\item User 1 answered Item 1 correct
\item User 1 answered Item 2 incorrect
\item User 2 answered Item 1 incorrect
\item User 2 answered Item 1 correct
\item User 2 answered Item 2 ???
\end{itemize}
\end{column}
\begin{column}{0.4\linewidth}
\centering
\input{tables/dummy-ui}\vspace{5mm}

\texttt{dummy.csv}
\end{column}
\end{columns}

## Crowdsourcing : apprendre de données d'humains $\to$ bruitées

\centering

\resizebox{0.9\linewidth}{!}{$\displaystyle \substack{\normalsize \Pr(\textnormal{"joueur A bat joueur B"})\\ \Pr(\textnormal{"étudiant A résout question B"})\\ \Pr(\textnormal{"A préféré à B"})} = \frac1{1 + \exp(-(score_A - score_B))}$}

\raggedright

Les gens affrontent des questions plus dures qu'eux (Rasch) et potentiellement apprennent au passage (Elo)

\begin{figure}
  \captionsetup[subfigure]{labelformat=empty,justification=centering}
  \subfloat[reCAPTCHA\\ (Luis von Ahn, 2008)]{\raisebox{2mm}{\includegraphics[width=0.25\linewidth]{figures/captcha.png}}}
  \subfloat[Elo (1967)\\ TrueSkill (2007)]{\includegraphics[width=0.25\linewidth]{figures/tournament-nyt.png}}
  \subfloat[Tests adaptatifs\\ (Rasch, 1960)]{\includegraphics[width=0.25\linewidth]{figures/irt.pdf}}
  \subfloat[Modèles de préférences\\ (Bradley \& Terry, 1952)]{\raisebox{3mm}{\includegraphics[width=0.25\linewidth]{figures/elo2.jpg}}}
\end{figure}

\vfill \small

\textcolor{gray}{Raykar, Yu, Zhao, Valadez, Florin, Bogoni, \& Moy (JMLR 2010).  
Learning from crowds.}

## Task 1: Item Response Theory

Learn abilities $\theta_i$ for each user $i$  
Learn easiness $e_j$ for each item $j$ such that:
$$ \begin{aligned}
Pr(\textnormal{User $i$ Item $j$ OK}) & = \sigma(\theta_i + e_j)\\
\logit Pr(\textnormal{User $i$ Item $j$ OK}) & = \theta_i + e_j
\end{aligned}$$

### Logistic regression

Learn $\alert{\bm{w}}$ such that $\logit Pr(\bm{x}) = \langle \alert{\bm{w}}, \bm{x} \rangle$

Usually with L2 regularization: ${||\bm{w}||}_2^2$ penalty $\leftrightarrow$ Gaussian prior

## Graphically: IRT as logistic regression

Encoding of "User $i$ answered Item $j$":

TODO inspi aaai2019 ktm slides

\centering

![](figures/lr.pdf)

$$ \logit Pr(\textnormal{User $i$ Item $j$ OK}) = \langle \bm{w}, \bm{x} \rangle = \theta_i + e_j $$

## Encoding

`python encode.py --users --items`  

\centering

\input{tables/show-ui}

`data/dummy/X-ui.npz`

\raggedright
Then logistic regression can be run on the sparse features:

`python lr.py data/dummy/X-ui.npz`

## Oh, there's a problem

`python encode.py --users --items`

`python lr.py data/dummy/X-ui.npz`

\input{tables/pred-ui}

We predict the same thing when there are several attempts.

## Count successes and failures

Keep track of what the student has done before:

\centering

\input{tables/dummy-uiswf}

`data/dummy/data.csv`

## Task 2: Performance Factor Analysis

$W_{ik}$: how many successes of user $i$ over skill $k$ ($F_{ik}$: #failures)

Learn $\alert{\beta_k}$, $\alert{\gamma_k}$, $\alert{\delta_k}$ for each skill $k$ such that:
$$ \logit Pr(\textnormal{User $i$ Item $j$ OK}) = \sum_{\textnormal{Skill } k \textnormal{ of Item } j} \alert{\beta_k} + W_{ik} \alert{\gamma_k} + F_{ik} \alert{\delta_k} $$

`python encode.py --skills --wins --fails`

\centering
\input{tables/show-swf}

`data/dummy/X-swf.npz`

## Better!

`python encode.py --skills --wins --fails`

`python lr.py data/dummy/X-swf.npz`

\input{tables/pred-swf}

## Task 3: a new model (but still logistic regression)

`python encode.py --items --skills --wins --fails`

`python lr.py data/dummy/X-iswf.npz`

# Multidimensional item response theory

## Code

```r
library(mirt)
```

## What can be done with embeddings?

\centering

![](figures/embedding1.png){width=60%}

## Interpreting the components

![](figures/embedding2.png)

## Interpreting the components

![](figures/embedding3.png)

## Graphically: logistic regression

\centering

![](figures/lr.pdf)

## How to model pairwise interactions with side information?

If you know user $i$ attempted item $j$ on \alert{mobile} (not desktop)  
How to model it?

$y$: score of event "user $i$ solves correctly item $j$"

### IRT

$$ y = \theta_i + e_j $$

### Multidimensional IRT (similar to collaborative filtering)

$$ y = \theta_i + e_j + \langle \bm{v_{\textnormal{user $i$}}}, \bm{v_{\textnormal{item $j$}}} \rangle $$

\pause

### With side information

\small \vspace{-3mm}
$$ y = \theta_i + e_j + \alert{w_{\textnormal{mobile}}} + \langle \bm{v_{\textnormal{user $i$}}}, \bm{v_{\textnormal{item $j$}}} \rangle + \langle \bm{v_{\textnormal{user $i$}}}, \alert{\bm{v_{\textnormal{mobile}}}} \rangle + \langle \bm{v_{\textnormal{item $j$}}}, \alert{\bm{v_{\textnormal{mobile}}}} \rangle $$

## Graphically: factorization machines

\centering

![](figures/fm.pdf)

![](figures/fm2.pdf)

# Deep Learning

## Deep Knowledge Tracing

LSTM of pairs (q_t, a_t)

Linear layer

Learn layers \alert{$W^{(\ell)}$} and \alert{$b^{(\ell)}$} such that:
$$ \begin{aligned}[c]
\bm{a}^{0}(\bm{x}) & = (\alert{\bm{v_{\texttt{user}}}}, \alert{\bm{v_{\texttt{item}}}}, \alert{\bm{v_{\texttt{skill}}}}, \ldots)\\
\bm{a}^{(\ell + 1)}(\bm{x}) & = \ReLU(\alert{W^{(\ell)}} \bm{a}^{(\ell)}(\bm{x}) + \alert{\bm{b}^{(\ell)}}) \quad \ell = 0, \ldots, L - 1\\
y_{DNN}(\bm{x}) & = \ReLU(\alert{W^{(L)}} \bm{a}^{(L)}(\bm{x}) + \alert{\bm{b}^{(L)}})
\end{aligned} $$

$$ \logit p(\bm{x}) = y_{FM}(\bm{x}) + y_{DNN}(\bm{x}) $$


## What 'bout recurrent neural networks?

Deep Knowledge Tracing: model the problem as sequence prediction

- Each student on skill $q_t$ has performance $a_t$
- How to predict outcomes $\bm{y}$ on every skill $k$?
- Spoiler: by measuring the evolution of a latent state $\alert{\bm{h_t}}$

\small
\fullcite{piech2015deep}
\normalsize

## Surprisingly

RQ1. IRT does better than DKT

TODO plot

## Surprisingly

\fullcite{wilson2016back}

RQ2. Random encoders perform not that bad

\fullcite{ding2019}

## Surprisingly

RQ3. DKT output KCs instead of items

\fullcite{gervet2020deep}

## Summarize and predict

$$h_t = E(q_{1:t}, a_{1:t}) p_t = D(h_t, q_t)$$

Fully connected layer is actually performing MIRT

### Our approach: encoder-decoder

\def\xin{\bm{x^{in}_t}}
\def\xout{\bm{x^{out}_t}}
$$\left\{\begin{array}{ll}
\bm{h_t} = Encoder(\bm{h_{t - 1}}, \xin)\\
p_t = Decoder(\bm{h_t}, \xout)\\
\end{array}\right. t = 1, \ldots, T$$

## Graphically: deep knowledge tracing

\centering

![](figures/dkt1.pdf)

## DKT seen as encoder-decoder

\centering

![](figures/dkt2.pdf)

## Results on Fraction dataset

500 middle-school students, 20 Fraction subtraction questions,  
8 skills (full matrix)

\begin{table}
\centering
\begin{tabular}{cccccc} \toprule
Model & Encoder & Decoder & $\xout$ & ACC & AUC\\ \midrule
\textbf{Ours} & GRU $d = 2$  & bias & iswf & \textbf{0.880} & \textbf{0.944}\\
KTM & counter & bias & iswf & 0.853 & 0.918\\
PFA & counter & bias & swf & 0.854 & 0.917\\
Ours & $\varnothing$  & bias & iswf & 0.849 & 0.917\\
Ours & GRU $d = 50$  & $\varnothing$ & & 0.814 & 0.880\\
DKT & GRU $d = 2$  & $d = 2$ & s & 0.772 & 0.844\\
Ours & GRU $d = 2$  & $\varnothing$ & & 0.751 & 0.800\\ \bottomrule
\end{tabular}
\label{results-fraction}
\end{table}

## Results on Berkeley dataset

562201 attempts of 1730 students over 234 CS-related items of 29 categories. 

\begin{table}
\centering
\begin{tabular}{cccccc} \toprule
Model & Encoder & Decoder & $\xout$ & ACC & AUC\\ \midrule
\textbf{Ours} & GRU $d = 50$ & bias & iswf & \textbf{0.707} & \textbf{0.778}\\
\textbf{KTM} & counter & bias & iswf & \textbf{0.704} & \textbf{0.775}\\
Ours & $\varnothing$ & bias & iswf & 0.700 & 0.770\\
DKT & GRU $d = 50$  & $d = 50$ & s & 0.684 & 0.751\\
Ours & GRU $d = 100$  & $\varnothing$ & & 0.682 & 0.750\\
PFA & counter & bias & swf & 0.630 & 0.683\\
DKT & GRU $d = 2$  & $d = 2$ & s & 0.637 & 0.656\\ \bottomrule
\end{tabular}
\label{results-assistments}
\end{table}

It is always better to have unidimensional or low-dimensional projection. Too big dimension overfits.

# Conclusion

## Take home message

\alert{Factorization machines} are a strong baseline for knowledge tracing that take many models as special cases

\alert{Recurrent neural networks} are powerful because they track the evolution of the latent state (try simpler dynamic models?)

\alert{Deep factorization machines} may require more data/tuning, but neural collaborative filtering offer promising directions

This is also true if we replace this with transformer (decoder-only architecture)

Next step: use this model and optimize human learning

All code available on GitHub

## We are organizing a optimizing human learning workshop at LAK 2024, Kyoto

Please submit papers by December 16.

Feel free to chat:

\centering
`vie@jill-jenn.net`

\raggedright
All code:

\centering
`github.com/jilljenn/dktm`
